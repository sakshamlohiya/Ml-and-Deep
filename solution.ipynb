{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WEEK 2 solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJcmCsz8rqn6"
      },
      "source": [
        "# **LET'S DIVE INTO SOME IMPLEMENTATION**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsuzsXIAsDxW"
      },
      "source": [
        "import numpy as np\n",
        "from numpy import array\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "import math\n",
        "import sklearn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hvmxh7YtYMW9"
      },
      "source": [
        "##Task 1 - Gradient Descent\n",
        " \n",
        "\n",
        "**Warm-up exercise**: Implement the gradient descent update rule. The  gradient descent rule is, for $l = 1, ..., L$: \n",
        "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{1}$$\n",
        "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{2}$$\n",
        "\n",
        "where L is the number of layers and $\\alpha$ is the learning rate. All parameters should be stored in the `parameters` dictionary. Note that the iterator `l` starts at 0 in the `for` loop while the first parameters are $W^{[1]}$ and $b^{[1]}$. You need to shift `l` to `l+1` when coding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZY4OvxDsk3Z"
      },
      "source": [
        "# GRADED FUNCTION: update_parameters_with_gd\n",
        "\n",
        "def update_parameters_with_gd(parameters, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Update parameters using one step of gradient descent\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters to be updated:\n",
        "                    parameters['W' + str(l)] = Wl\n",
        "                    parameters['b' + str(l)] = bl\n",
        "    grads -- python dictionary containing your gradients to update each parameters:\n",
        "                    grads['dW' + str(l)] = dWl\n",
        "                    grads['db' + str(l)] = dbl\n",
        "    learning_rate -- the learning rate, scalar.\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "    \"\"\"\n",
        "\n",
        "    L = len(parameters) // 2 # number of layers in the neural networks\n",
        "\n",
        "    # Update rule for each parameter\n",
        "    for l in range(L):\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "\n",
        "        parameters[\"W\" + str(l+1)] =parameters[\"W\" + str(l+1)] - learning_rate * grads['dW' + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] =parameters[\"b\" + str(l+1)] - learning_rate * grads['db' + str(l+1)]\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "    return parameters"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jG3XF_oTzD4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28064aad-879a-43ea-c632-8a3dce5869dc"
      },
      "source": [
        "\n",
        "#assign the value of parameters and grads.Do it in this cell itself so that the resulting parameters don't change every time you run this cell\n",
        "parameters={'W1': array([[ 1.39134536, -0.61175641, -0.52817175],\n",
        "         [-1.07296862,  0.86540763, -2.7815387 ]]),\n",
        "  'W2': array([[ 0.3190391 , -0.24937038,  1.46210794],\n",
        "         [-2.06014071, -0.3224172 , -0.38405435],\n",
        "         [ 1.13376944, -1.09989127, -0.17242821]]),\n",
        "  'b1': array([[ 1.74481176],\n",
        "         [-0.7612069 ]]),\n",
        "  'b2': array([[-0.87785842],\n",
        "         [ 0.04221375],\n",
        "         [ 0.58281521]])}\n",
        "grads={'dW1': array([[-1.10061918,  1.14472371,  0.90159072],\n",
        "         [ 0.50249434,  0.90085595, -0.68372786]]),\n",
        "  'dW2': array([[-0.21788808,  0.53035547, -0.69166075],\n",
        "         [-0.39675353, -0.6871727 , -0.84520564],\n",
        "         [-0.67124613, -0.0126646 , -1.11731035]]),\n",
        "  'db1': array([[-0.12289023],\n",
        "         [-0.93576943]]),\n",
        "  'db2': array([[ 0.2844157 ],\n",
        "         [ 1.78980218],\n",
        "         [ 0.79204416]])}\n",
        "\n",
        "\n",
        "parameters = update_parameters_with_gd(parameters, grads, learning_rate=0.001)\n",
        "print(\"W1 =\\n\" + str(parameters[\"W1\"]))\n",
        "print(\"b1 =\\n\" + str(parameters[\"b1\"]))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1 =\n",
            "[[ 1.39244598 -0.61290113 -0.52907334]\n",
            " [-1.07347111  0.86450677 -2.78085497]]\n",
            "b1 =\n",
            "[[ 1.74493465]\n",
            " [-0.76027113]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUeh9L9IR9dB"
      },
      "source": [
        "# SECOND TASK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS25RJWZWD5h"
      },
      "source": [
        "## 3 - Momentum\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Exercise**: Initialize the velocity. The velocity, $v$, is a python dictionary that needs to be initialized with arrays of zeros. Its keys are the same as those in the `grads` dictionary, that is:\n",
        "for $l =1,...,L$:\n",
        "```python\n",
        "v[\"dW\" + str(l+1)] = ... #(numpy array of zeros with the same shape as parameters[\"W\" + str(l+1)])\n",
        "v[\"db\" + str(l+1)] = ... #(numpy array of zeros with the same shape as parameters[\"b\" + str(l+1)])\n",
        "```\n",
        "**Note** that the iterator l starts at 0 in the for loop while the first parameters are v[\"dW1\"] and v[\"db1\"] (that's a \"one\" on the superscript). This is why we are shifting l to l+1 in the `for` loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0pdLuxhTklP"
      },
      "source": [
        "def initialize_velocity(parameters):\n",
        "    \"\"\"\n",
        "    Initializes the velocity as a python dictionary with:\n",
        "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
        "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters.\n",
        "                    parameters['W' + str(l)] = Wl\n",
        "                    parameters['b' + str(l)] = bl\n",
        "    \n",
        "    Returns:\n",
        "    v -- python dictionary containing the current velocity.\n",
        "                    v['dW' + str(l)] = velocity of dWl\n",
        "                    v['db' + str(l)] = velocity of dbl\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural networks\n",
        "    v = {}\n",
        "    \n",
        "    # Initialize velocity\n",
        "    for l in range(L):\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "        v[\"dW\" + str(l+1)] = np.zeros_like(parameters['W' + str(l+1)])\n",
        "        v[\"db\" + str(l+1)] = np.zeros_like(parameters['b' + str(l+1)])\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "    return v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiFsvSzsUQi6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdf08f4e-c4d4-4f2b-879f-f7eeb49c4ceb"
      },
      "source": [
        "\n",
        "v = initialize_velocity(parameters)\n",
        "print(\"v[\\\"dW1\\\"] =\\n\" + str(v[\"dW1\"]))\n",
        "print(\"v[\\\"db1\\\"] =\\n\" + str(v[\"db1\"]))\n",
        "print(\"v[\\\"dW2\\\"] =\\n\" + str(v[\"dW2\"]))\n",
        "print(\"v[\\\"db2\\\"] =\\n\" + str(v[\"db2\"]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "v[\"dW1\"] =\n",
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "v[\"db1\"] =\n",
            "[[0.]\n",
            " [0.]]\n",
            "v[\"dW2\"] =\n",
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "v[\"db2\"] =\n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YBvFDvZaOiF"
      },
      "source": [
        "**Expected Output**:\n",
        "\n",
        "```\n",
        "v[\"dW1\"] =\n",
        "[[ 0.  0.  0.]\n",
        " [ 0.  0.  0.]]\n",
        "v[\"db1\"] =\n",
        "[[ 0.]\n",
        " [ 0.]]\n",
        "v[\"dW2\"] =\n",
        "[[ 0.  0.  0.]\n",
        " [ 0.  0.  0.]\n",
        " [ 0.  0.  0.]]\n",
        "v[\"db2\"] =\n",
        "[[ 0.]\n",
        " [ 0.]\n",
        " [ 0.]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZbbaF8aV0_E"
      },
      "source": [
        "**Exercise**:  Now, implement the parameters update with momentum. The momentum update rule is, for $l = 1, ..., L$: \n",
        "\n",
        "$$ \\begin{cases}\n",
        "v_{dW^{[l]}} = \\beta v_{dW^{[l]}} + (1 - \\beta) dW^{[l]} \\\\\n",
        "W^{[l]} = W^{[l]} - \\alpha v_{dW^{[l]}}\n",
        "\\end{cases}\\tag{3}$$\n",
        "\n",
        "$$\\begin{cases}\n",
        "v_{db^{[l]}} = \\beta v_{db^{[l]}} + (1 - \\beta) db^{[l]} \\\\\n",
        "b^{[l]} = b^{[l]} - \\alpha v_{db^{[l]}} \n",
        "\\end{cases}\\tag{4}$$\n",
        "\n",
        "where L is the number of layers, $\\beta$ is the momentum and $\\alpha$ is the learning rate. All parameters should be stored in the `parameters` dictionary.  Note that the iterator `l` starts at 0 in the `for` loop while the first parameters are $W^{[1]}$ and $b^{[1]}$ (that's a \"one\" on the superscript). So you will need to shift `l` to `l+1` when coding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XarLZWrHUk2d"
      },
      "source": [
        "def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n",
        "    \"\"\"\n",
        "    Update parameters using Momentum\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters:\n",
        "                    parameters['W' + str(l)] = Wl\n",
        "                    parameters['b' + str(l)] = bl\n",
        "    grads -- python dictionary containing your gradients for each parameters:\n",
        "                    grads['dW' + str(l)] = dWl\n",
        "                    grads['db' + str(l)] = dbl\n",
        "    v -- python dictionary containing the current velocity:\n",
        "                    v['dW' + str(l)] = ...\n",
        "                    v['db' + str(l)] = ...\n",
        "    beta -- the momentum hyperparameter, scalar\n",
        "    learning_rate -- the learning rate, scalar\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "    v -- python dictionary containing your updated velocities\n",
        "    \"\"\"\n",
        "\n",
        "    L = len(parameters) // 2 # number of layers in the neural networks\n",
        "    \n",
        "    # Momentum update for each parameter\n",
        "    for l in range(L):\n",
        "        \n",
        "        ### START CODE HERE ### (approx. 4 lines)\n",
        "        # compute velocities\n",
        "        v[\"dW\" + str(l+1)] = beta * v[\"dW\" + str(l+1)] + (1-beta) * grads['dW' + str(l+1)]\n",
        "        v[\"db\" + str(l+1)] = beta * v[\"db\" + str(l+1)] + (1-beta) * grads['db' + str(l+1)]\n",
        "        # update parameters\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * v['dW' + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * v['db' + str(l+1)]\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "    return parameters, v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iVLvFahZS9F"
      },
      "source": [
        "The parameters and grads are the same as in task 1 and v is the same as earlier i.e., initialized to zero.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlsHfn0H93W7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c43d81da-e403-4e68-c21c-1111defa0498"
      },
      "source": [
        "#assign the values of arguments\n",
        "parameters={'W1': array([[ 1.39134536, -0.61175641, -0.52817175],\n",
        "         [-1.07296862,  0.86540763, -2.7815387 ]]),\n",
        "  'W2': array([[ 0.3190391 , -0.24937038,  1.46210794],\n",
        "         [-2.06014071, -0.3224172 , -0.38405435],\n",
        "         [ 1.13376944, -1.09989127, -0.17242821]]),\n",
        "  'b1': array([[ 1.74481176],\n",
        "         [-0.7612069 ]]),\n",
        "  'b2': array([[-0.87785842],\n",
        "         [ 0.04221375],\n",
        "         [ 0.58281521]])}\n",
        "grads={'dW1': array([[-1.10061918,  1.14472371,  0.90159072],\n",
        "         [ 0.50249434,  0.90085595, -0.68372786]]),\n",
        "  'dW2': array([[-0.21788808,  0.53035547, -0.69166075],\n",
        "         [-0.39675353, -0.6871727 , -0.84520564],\n",
        "         [-0.67124613, -0.0126646 , -1.11731035]]),\n",
        "  'db1': array([[-0.12289023],\n",
        "         [-0.93576943]]),\n",
        "  'db2': array([[ 0.2844157 ],\n",
        "         [ 1.78980218],\n",
        "         [ 0.79204416]])}\n",
        "#run the function\n",
        "\n",
        "params, v0=update_parameters_with_momentum(parameters, grads, v, beta=0.9, learning_rate=0.01)\n",
        "v0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dW1': array([[-0.11006192,  0.11447237,  0.09015907],\n",
              "        [ 0.05024943,  0.09008559, -0.06837279]]),\n",
              " 'dW2': array([[-0.02178881,  0.05303555, -0.06916607],\n",
              "        [-0.03967535, -0.06871727, -0.08452056],\n",
              "        [-0.06712461, -0.00126646, -0.11173103]]),\n",
              " 'db1': array([[-0.01228902],\n",
              "        [-0.09357694]]),\n",
              " 'db2': array([[0.02844157],\n",
              "        [0.17898022],\n",
              "        [0.07920442]])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irfqZlBti3MA"
      },
      "source": [
        "Learning rate has no impact on v ,changing it with same parameters would never affect v .Learning rate only updates the parameters and hence affects them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLuHthX4X7nM"
      },
      "source": [
        "END OF ASSIGNMENT\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}